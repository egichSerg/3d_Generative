{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "585b462c-ef4f-45e5-beb7-cefff5c96191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import img_as_ubyte\n",
    "from pytorch3d.io import load_objs_as_meshes\n",
    "from pytorch3d.renderer import (\n",
    "FoVPerspectiveCameras, look_at_view_transform,\n",
    "look_at_rotation, RasterizationSettings,\n",
    "MeshRenderer, MeshRasterizer, BlendParams,\n",
    "SoftSilhouetteShader, HardPhongShader,\n",
    "PointLights, SoftPhongShader\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import imageio, PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ad51861-72f4-46c4-848b-b97888ff907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e201591-233e-4dbc-9e5a-3c66efcc12a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path('./result_cow')\n",
    "data_dir = Path('./data/cow_mesh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a602f38e-6d89-459c-aebc-726979158cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_filename = data_dir / 'cow.obj'\n",
    "cow_mesh = load_objs_as_meshes([obj_filename], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "964e5ed6-ff9f-4750-853f-84aecdb26ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = FoVPerspectiveCameras(device=device)\n",
    "lights = PointLights(device=device, location=((2.0, 2.0, -2.0), ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c97e4f-7e59-4a39-bdbe-6928c6ee910c",
   "metadata": {},
   "source": [
    "### Силуэтный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "859facfd-fe1e-4a9c-9005-3bb5fadeefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_params = BlendParams(sigma=1e-4, gamma=1e-4)\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=256,\n",
    "    blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma,\n",
    "    faces_per_pixel=100,\n",
    ")\n",
    "renderer_silhouette = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "            cameras=cameras,\n",
    "            raster_settings=raster_settings\n",
    "        ),\n",
    "    shader=SoftSilhouetteShader(blend_params=blend_params)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a311014c-04ad-4e9e-80a1-833c5271aeaf",
   "metadata": {},
   "source": [
    "### Текстурный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0cec36b-eefd-4675-ba9a-fa27bb2d8e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1e-4\n",
    "raster_settings_soft = RasterizationSettings(\n",
    "        image_size=256,\n",
    "        blur_radius=np.log(1. / 1e-4 - 1.)*sigma,\n",
    "        faces_per_pixel=50,\n",
    "    )\n",
    "renderer_textured = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras,\n",
    "        raster_settings=raster_settings_soft\n",
    "    ),\n",
    "    shader=SoftPhongShader(\n",
    "        device=device,\n",
    "        cameras=cameras,\n",
    "        lights=lights\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61275732-1fd9-4e07-8aec-54db21066e91",
   "metadata": {},
   "source": [
    "### Для визуализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cde253e-257b-430b-8f90-310d0edf6c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_settings = RasterizationSettings(\n",
    "        image_size=256,\n",
    "        blur_radius=0.0,\n",
    "        faces_per_pixel=1,\n",
    "    )\n",
    "phong_renderer = MeshRenderer(\n",
    "        rasterizer=MeshRasterizer(\n",
    "            cameras=cameras,\n",
    "            raster_settings=raster_settings\n",
    "        ),\n",
    "            shader=HardPhongShader(\n",
    "            device=device,\n",
    "            cameras=cameras,\n",
    "            lights=lights\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d27550-63b0-4300-9a9b-be542fbafa13",
   "metadata": {},
   "source": [
    "### Камера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d43312e-01f3-444b-b04b-1ea1faaf7d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = 3\n",
    "elevation = 50.0\n",
    "azimuth = 0.0\n",
    "R, T = look_at_view_transform(distance,\n",
    "                              elevation,\n",
    "                              azimuth,\n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fdc2ad3-e46a-4c83-be0e-6664e9583b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette = renderer_silhouette(meshes_world=cow_mesh,\n",
    "                                 R=R, T=T)\n",
    "image_ref = phong_renderer(meshes_world=cow_mesh,\n",
    "                           R=R, T=T)\n",
    "\n",
    "silhouette = silhouette.cpu().numpy()\n",
    "image_ref = image_ref.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(silhouette.squeeze()[..., 3])\n",
    "plt.grid(False)\n",
    "plt.savefig(os.path.join(output_dir, 'target_silhouette.png'))\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image_ref.squeeze())\n",
    "plt.grid(False)\n",
    "plt.savefig(os.path.join(output_dir, 'target_rgb.png'))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74b2f4-b9e1-4440-99fc-bd06c275aacc",
   "metadata": {},
   "source": [
    "### Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2144272f-663c-4b10-a62a-a37c2947353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, meshes, renderer_silhouette, renderer_textured, image_ref, weight_silhouette, weight_texture):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.meshes = meshes\n",
    "        self.device = meshes.device\n",
    "        self.renderer_silhouette = renderer_silhouette\n",
    "        self.renderer_textured = renderer_textured\n",
    "        self.weight_silhouette = weight_silhouette\n",
    "        self.weight_texture = weight_texture\n",
    "        \n",
    "        image_ref_silhouette = torch.from_numpy((image_ref[..., :3].max(-1) != 1).astype(np.float32))\n",
    "        self.register_buffer('image_ref_silhouette', image_ref_silhouette)\n",
    "        \n",
    "        image_ref_textured = torch.from_numpy((image_ref[..., :3]).astype(np.float32))\n",
    "        self.register_buffer('image_ref_textured', image_ref_textured)\n",
    "        \n",
    "        self.camera_position = nn.Parameter(torch.from_numpy(np.array([3.0, 6.9, +2.5], dtype=np.float32)).to(meshes.device))\n",
    "        \n",
    "    def forward(self):\n",
    "        R = look_at_rotation( self.camera_position[None, :], device=self.device) # (1, 3, 3)\n",
    "        T = -torch.bmm(\n",
    "            R.transpose(1, 2),\n",
    "            self.camera_position[None, :, None])[:, :, 0] # (1, 3)\n",
    "        \n",
    "        image_silhouette = self.renderer_silhouette(meshes_world=self.meshes.clone(), R=R, T=T)\n",
    "        image_textured = self.renderer_textured(meshes_world=self.meshes.clone(), R=R, T=T)\n",
    "        \n",
    "        loss_silhouette = torch.sum((image_silhouette[..., 3] - self.image_ref_silhouette) ** 2)\n",
    "        loss_texture = torch.sum((image_textured[..., :3] - self.image_ref_textured) ** 2)\n",
    "        loss = self.weight_silhouette * loss_silhouette + self.weight_texture * loss_texture\n",
    "        return loss, image_silhouette, image_textured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34722100-600f-463b-bd7c-9dccc535c55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(meshes=cow_mesh,\n",
    "              renderer_silhouette=renderer_silhouette,\n",
    "              renderer_textured=renderer_textured,\n",
    "              image_ref=image_ref, weight_silhouette=1.0,\n",
    "              weight_texture=0.1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afef8912-e5e1-4426-b57d-80c201d8a28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [01:48<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "images = []\n",
    "for i in tqdm(range(epochs)):        \n",
    "    loss, image_silhouette, image_textured = model()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(image_silhouette[..., 3].detach().squeeze().cpu().numpy())\n",
    "    plt.title(\"iter: %d, loss: %0.2f\" % (i, loss.data))\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(os.path.join(output_dir, 'soft_silhouette_' + str(i) + '.png'))\n",
    "    plt.close()\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.imshow(image_textured.detach().squeeze().cpu().numpy())\n",
    "    plt.title(\"iter: %d, loss: %0.2f\" % (i, loss.data))\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(os.path.join(output_dir,\n",
    "    'soft_texture_' + str(i) + '.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    R = look_at_rotation(model.camera_position[None, :], device=model.device)\n",
    "    T = -torch.bmm(R.transpose(1, 2), model.camera_position[None, :, None])[:, :, 0] # (1, 3)\n",
    "    image = phong_renderer(meshes_world=model.meshes.clone(), R=R, T=T)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(image[..., 3].detach().squeeze().cpu().numpy())\n",
    "    plt.title(\"iter: %d, loss: %0.2f\" % (i, loss.data))\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(os.path.join(output_dir, 'hard_silhouette_' + str(i) + '.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    image = image[0, ..., :3].detach().squeeze().cpu().numpy()\n",
    "    image = img_as_ubyte(image)\n",
    "    images.append(image)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(image[..., :3])\n",
    "    plt.title(\"iter: %d, loss: %0.2f\" % (i, loss.data))\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(os.path.join(output_dir, 'hard_texture_' + str(i) + '.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    if loss.item() < 800:\n",
    "        break\n",
    "\n",
    "\n",
    "imageio.mimsave(output_dir / 'cow.gif', images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cc5662-7189-4d9e-be9c-5eb44829746a",
   "metadata": {},
   "source": [
    "### Попробуем исходный код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1150839-5972-45cb-9ec7-31972db09670",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (256) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 137\u001b[0m\n\u001b[1;32m    132\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(meshes\u001b[38;5;241m=\u001b[39mcow_mesh, renderer_silhouette\u001b[38;5;241m=\u001b[39mrenderer_silhouette, renderer_textured \u001b[38;5;241m=\u001b[39m renderer_textured,\n\u001b[1;32m    133\u001b[0m               image_ref\u001b[38;5;241m=\u001b[39mimage_ref, weight_silhouette\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, weight_texture\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    135\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m--> 137\u001b[0m _, image_silhouette_init, image_rgb_init \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m    139\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(image_silhouette_init\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 127\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m image_textured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenderer_textured(meshes_world\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeshes\u001b[38;5;241m.\u001b[39mclone(), R\u001b[38;5;241m=\u001b[39mR, T\u001b[38;5;241m=\u001b[39mT)\n\u001b[1;32m    126\u001b[0m loss_silhouette \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((image_silhouette[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_ref_silhouette) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m loss_texture \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((\u001b[43mimage_textured\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_ref_textured\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    129\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_silhouette \u001b[38;5;241m*\u001b[39m loss_silhouette \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_texture \u001b[38;5;241m*\u001b[39m loss_texture\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, image_silhouette, image_textured\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (256) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "from pytorch3d.io import load_objs_as_meshes\n",
    "\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras, look_at_view_transform, look_at_rotation,\n",
    "    RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n",
    "    SoftSilhouetteShader, HardPhongShader, PointLights,\n",
    "    SoftPhongShader\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "output_dir = './result_cow'\n",
    "\n",
    "obj_filename = \"./data/cow_mesh/cow.obj\"\n",
    "cow_mesh = load_objs_as_meshes([obj_filename], device=device)\n",
    "\n",
    "cameras = FoVPerspectiveCameras(device=device)\n",
    "lights = PointLights(device=device, location=((2.0, 2.0, -2.0),))\n",
    "\n",
    "blend_params = BlendParams(sigma=1e-4, gamma=1e-4)\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=128,\n",
    "    blur_radius=np.log(1. / 1e-4 - 1.) * blend_params.sigma,\n",
    "    faces_per_pixel=100,\n",
    ")\n",
    "\n",
    "renderer_silhouette = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras,\n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftSilhouetteShader(blend_params=blend_params)\n",
    ")\n",
    "\n",
    "sigma = 1e-4\n",
    "raster_settings_soft = RasterizationSettings(\n",
    "    image_size=128,\n",
    "    blur_radius=np.log(1. / 1e-4 - 1.)*sigma,\n",
    "    faces_per_pixel=50,\n",
    ")\n",
    "renderer_textured = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras,\n",
    "        raster_settings=raster_settings_soft\n",
    "    ),\n",
    "    shader=SoftPhongShader(device=device,\n",
    "        cameras=cameras,\n",
    "        lights=lights)\n",
    ")\n",
    "\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=128,\n",
    "    blur_radius=0.0,\n",
    "    faces_per_pixel=1,\n",
    ")\n",
    "phong_renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras,\n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=HardPhongShader(device=device, cameras=cameras, lights=lights)\n",
    ")\n",
    "\n",
    "distance = 3\n",
    "elevation = 50.0\n",
    "azimuth = 0.0\n",
    "R, T = look_at_view_transform(distance, elevation, azimuth, device=device)\n",
    "\n",
    "silhouette = renderer_silhouette(meshes_world=cow_mesh, R=R, T=T)\n",
    "image_ref = phong_renderer(meshes_world=cow_mesh, R=R, T=T)\n",
    "silhouette = silhouette.cpu().numpy()\n",
    "image_ref = image_ref.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(silhouette.squeeze()[..., 3])\n",
    "plt.grid(False)\n",
    "plt.savefig(os.path.join(output_dir, 'target_silhouette.png'))\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image_ref.squeeze())\n",
    "plt.grid(False)\n",
    "plt.savefig(os.path.join(output_dir, 'target_rgb.png'))\n",
    "plt.close()\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, meshes, renderer_silhouette, renderer_textured, image_ref, weight_silhouette, weight_texture):\n",
    "        super().__init__()\n",
    "        self.meshes = meshes\n",
    "        self.device = meshes.device\n",
    "        self.renderer_silhouette = renderer_silhouette\n",
    "        self.renderer_textured = renderer_textured\n",
    "\n",
    "        self.weight_silhouette = weight_silhouette\n",
    "        self.weight_texture = weight_texture\n",
    "\n",
    "        image_ref_silhouette = torch.from_numpy((image_ref[..., :3].max(-1) != 1).astype(np.float32))\n",
    "        self.register_buffer('image_ref_silhouette', image_ref_silhouette)\n",
    "\n",
    "        image_ref_textured = torch.from_numpy((image_ref[..., :3]).astype(np.float32))\n",
    "        self.register_buffer('image_ref_textured', image_ref_textured)\n",
    "\n",
    "        self.camera_position = nn.Parameter(\n",
    "            torch.from_numpy(np.array([3.0, 6.9, +2.5], dtype=np.float32)).to(meshes.device))\n",
    "\n",
    "    def forward(self):\n",
    "        # Render the image using the updated camera position. Based on the new position of the\n",
    "        # camera we calculate the rotation and translation matrices\n",
    "        R = look_at_rotation(self.camera_position[None, :], device=self.device)  # (1, 3, 3)\n",
    "        T = -torch.bmm(R.transpose(1, 2), self.camera_position[None, :, None])[:, :, 0]  # (1, 3)\n",
    "\n",
    "        image_silhouette = self.renderer_silhouette(meshes_world=self.meshes.clone(), R=R, T=T)\n",
    "        image_textured = self.renderer_textured(meshes_world=self.meshes.clone(), R=R, T=T)\n",
    "\n",
    "        loss_silhouette = torch.sum((image_silhouette[..., 3] - self.image_ref_silhouette) ** 2)\n",
    "        loss_texture = torch.sum((image_textured[..., :3] - self.image_ref_textured) ** 2)\n",
    "\n",
    "        loss = self.weight_silhouette * loss_silhouette + self.weight_texture * loss_texture\n",
    "        return loss, image_silhouette, image_textured\n",
    "\n",
    "model = Model(meshes=cow_mesh, renderer_silhouette=renderer_silhouette, renderer_textured = renderer_textured,\n",
    "              image_ref=image_ref, weight_silhouette=1.0, weight_texture=0.1).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "_, image_silhouette_init, image_rgb_init = model()\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image_silhouette_init.detach().squeeze().cpu().numpy()[..., 3])\n",
    "plt.grid(False)\n",
    "plt.title(\"Starting Silhouette\")\n",
    "plt.savefig(os.path.join(output_dir, 'starting_silhouette.png'))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image_rgb_init.cpu().detach().numpy().squeeze())\n",
    "plt.grid(False)\n",
    "plt.title(\"Starting RGB Image\");\n",
    "plt.savefig(os.path.join(output_dir, 'starting_rgb.png'))\n",
    "\n",
    "for i in range(0, 200):\n",
    "    if i%10 == 0:\n",
    "        print('i = ', i)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss, image_silhouette, image_textured = model()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(image_silhouette[..., 3].detach().squeeze().cpu().numpy())\n",
    "    plt.title(\"iter: %d, loss: %0.2f\" % (i, loss.data))\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(os.path.join(output_dir, 'soft_silhouette_' + str(i) + '.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(image_textured.detach().squeeze().cpu().numpy())\n",
    "    plt.title(\"iter: %d, loss: %0.2f\" % (i, loss.data))\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(os.path.join(output_dir, 'soft_texture_' + str(i) + '.png'))\n",
    "    plt.close()\n",
    "\n",
    "    R = look_at_rotation(model.camera_position[None, :], device=model.device)\n",
    "    T = -torch.bmm(R.transpose(1, 2), model.camera_position[None, :, None])[:, :, 0]  # (1, 3)\n",
    "    image = phong_renderer(meshes_world=model.meshes.clone(), R=R, T=T)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(image[..., 3].detach().squeeze().cpu().numpy())\n",
    "    plt.title(\"iter: %d, loss: %0.2f\" % (i, loss.data))\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(os.path.join(output_dir, 'hard_silhouette_' + str(i) + '.png'))\n",
    "    plt.close()\n",
    "\n",
    "    image = image[0, ..., :3].detach().squeeze().cpu().numpy()\n",
    "    image = img_as_ubyte(image)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(image[..., :3])\n",
    "    plt.title(\"iter: %d, loss: %0.2f\" % (i, loss.data))\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(os.path.join(output_dir, 'hard_texture_' + str(i) + '.png'))\n",
    "    plt.close()\n",
    "\n",
    "    if loss.item() < 800:\n",
    "        break\n",
    "\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab306fb2-cbc0-44ce-9d92-a20e944c9423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
